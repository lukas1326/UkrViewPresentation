fitControl <- trainControl(method = "cv",
number = 5)
fit.rf <- train(x,y, method="rf",data=train,trControl = fitControl,metric=metric)
stopCluster(cluster)
registerDoSEQ()
fit.rf$resample
confusionMatrix.train(fit.rf)
# a) linear algorithms
predTest <- predict(fit.rf,test)
confusionMatrix(predTest, test$classe)
##levels(clean2$cvtd_timestamp)<-levels(clean1$cvtd_timestamp)
result <- predict(fit.rf,clean2[,-53])
result
print(data.frame(clean2$problem_id,result))
print(result))
print(result)
print(vector(result))
print(as.vector(result))
print(as.list(result))
print(relevels(as.list(result)))
print(relevel(as.list(result)))
print(as.list(result)[1])
print(as.list(result)[,1])
as.list.data.frame(result)
as.array(result)
as.vector.factor(result)
dafa.frame(colnames(clean2$problem_id),as.vector.factor(result))
colnames(clean2$problem_id)
dafa.frame(colnames=clean2$problem_id,as.vector.factor(result))
?data.frame
do.call(clean2$problem_id,result)
do.call(clean2$problem_id,as.list(result))
do.call(clean2$problem_id,as.vestor.factor(result))
do.call(clean2$problem_id,as.character(result))
do.call(clean2$problem_id,as.vector.factor(result))
as.vector.factor(result)
as.list.factor(result)
do.call(clean2$problem_id,as.list.factor(result))
do.call(clean2$problem_id,result)
relevel(result)
as.character.factor(result)
do.call(clean2$problem_id,as.character.factor(result))
?do.call
do.call(rbind,result)
do.call(rbind,as.list(result))
do.call(rbind,as.list.factor(result))
do.call(rbind,as.vector.factor(result))
do.call(rbind,as.character(result))
expand.grid(result)
?expand.model.frame(result)
expand.model.frame(result)
?expand.grid
expand.grid(result,KEEP.OUT.ATTRS = FALSE)
expand.grid(result,KEEP.OUT.ATTRS = FALSE,stringsAsFactors = FALSE)
transform.data.frame(expand.grid(result))
transform.data.frame(expand.grid(result),var1)
?transform
transmute(expand.grid(result))
expand.grid(result)
fit.rf$resample
train_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(train_url,"data/training.csv")
download.file(test_url,"data/testing.csv")
confusionMatrix(fit.rf,test$classe)$overall[1]
knitr::opts_chunk$set(echo = TRUE,message = FALSE,warning = FALSE)
library(tidyverse)
library(caret)
library(parallel)
library(doParallel)
train_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(train_url,"data/training.csv")
download.file(test_url,"data/testing.csv")
setwd("~/datasciencecoursera/Statistic/MachineLearning/ML_course_project")
training <- read.csv("data/training.csv")
testing <- read.csv("data/testing.csv")
percentage <- prop.table(table(training$classe))*100
cbind(freq=table(training$classe), percentage= percentage)
## drop culculated columns
calstat <- c("kurtosis", "skewness", "max", "min", "amplitude", "var", "avg", "stddev")
dropstat <- unique(grep(paste(calstat,collapse = "|"),names(training)))
##variables with near zero variance
nz <- nearZeroVar(training[,-dropstat])
##data frames for modeling and testing with the same dimention
clean1 <- training[,-c(dropstat,nz,3,4,1,2,5,6,7)]
clean2 <- testing[,-c(dropstat,nz,3,4,1,2,5,6,7)]
# create training & testing data sets
inTraining <- createDataPartition(clean1$classe, p = .75, list=FALSE)
train <- clean1[inTraining,]
test <- clean1[-inTraining,]
# set up training run for x / y
x <- train[,-53]
y <- train[,53]
cluster <- makeCluster(detectCores())
registerDoParallel(cluster)
metric <- "Accuracy"
fitControl <- trainControl(method = "cv",
number = 5)
fit.rf <- train(x,y, method="rf",data=train,trControl = fitControl,metric=metric)
stopCluster(cluster)
registerDoSEQ()
fit.rf$resample
confusionMatrix.train(fit.rf)
#checking model on the sub-test
predTest <- predict(fit.rf,test)
confusionMatrix(predTest, test$classe)
result <- predict(fit.rf,clean2[,-53])
expand.grid(result)
confusionMatrix(fit.rf,test$classe)$overall[1]
confusionMatrix(predTest,test$classe)$overall[1]
ku <-confusionMatrix(fit.rf,test$classe)
ku <-confusionMatrix(predTest,test$classe)
View(ku)
ku[["table"]]
confusionMatrix(predTest, test$classe)[["table"]]
confusionMatrix(predtest),test$classe)$overall[1]
confusionMatrix(predtest,test$classe)$overall[1]
confusionMatrix(predTest,test$classe)$overall[1]
confusionMatrix(predTest,test$classe)$overall[[2]]
Sys.setlocale("LC_ALL",locale = "Ukrainian")
qu <- "В склад рішення включені комп'ютерні програми, майнові права на які належать БКС, а саме: (1) Титан, (2) Титан-СТВОР, (3) Титан-топографія, (4) Titan-multiview, (5) Titan-database, (6) Titan-parser. Всі майнові права інтелектуальної власності (авторські майнові права) на зазначені КП зареєстровані, отримані відповідні рішення та свідоцтва
"
source("ListFromStringFunction.R")
nazvaniya <-function(x) {
result<-str_match_all(x,"(\\d)\\)\\s(\\D+)\\s")
numbers <- result[[1]][,2]
list <- result[[1]][,3]
df <- cbind(numbers,list)
}
z<-nazvaniya(qu)
library(stringr)
nazvaniya <-function(x) {
result<-str_match_all(x,"(\\d)\\)\\s(\\D+)\\s")
numbers <- result[[1]][,2]
list <- result[[1]][,3]
df <- cbind(numbers,list)
}
library(stringr)
z<-nazvaniya(qu)
en_files <- list.files("/data/final/en_US/")
nlines<-numeric()
for (i in 1:3){
con <- file(paste0("/data/final/en_US/",en_files[i]), open = "r")
nlines[i] <- length(readLines(con,skipNul = TRUE))
close(con)
}
twit_subset <- vector("list", 3)
for (i in 1:3){
con <- file(paste0("/data/final/en_US/",en_files[i]), open = "r")
twit_subset[[i]] <- readLines(con,skipNul = TRUE)[which(rbinom(nlines[i],1,0.5)==1)]
close(con)
}
head(twit_subset[1])
twit_subset[[1]]
text_df <- data_frame(line = 1:length(twit_subset[[1]]), text = twit_subset[[1]])
library(tidyverse)
library(tidytext)
text_df <- data_frame(line = 1:length(twit_subset[[1]]), text = twit_subset[[1]])
head(text_df)
text_df <- text_df %>% mutate(n_set=1)
head(text_df)
tidy_twit <- text_df %>%
unnest_tokens(word, text)
View(tidy_twit)
tidy_twit <- tidy_twit %>%
anti_join(stop_words)
library(magrittr)
foo_too <- little_bunny()
pryr:object_size(text_df)
install.packages("pryr")
pryr:object_size(text_df)
pryr::object_size(text_df)
pryr::object_size(text_df,tidy_twit,twit_subset)
rnorm(100) %>%
matrix(ncol=2) %>%
plot() %>%
str()
rnorm(100) %>%
matrix(ncol=2) %>%
plot() %>%
str()
rnorm(100) %>%
matrix(ncol=2) %>%
plot() %>%
str()
pryr::object_size(text_df,tidy_twit,twit_subset)
install.packages("pryr")
rnorm(100) %>%
matrix(ncol=2) %T>%
plot() %>%
str()
install.packages("pryr")
install.packages("pryr")
rnorm(100) %>%
matrix(ncol=2) %T>%
plot() %>%
str()
install.packages("pryr")
rnorm(100) %>%
matrix(ncol=2) %T>%
plot() %>%
str()
url<-"https://privat24.privatbank.ua/p24/web/#archive-menu-app/kommunalka"
names=read_html(url)
library(rvest)
library(htmltools)
url<-"https://privat24.privatbank.ua/p24/web/#archive-menu-app/kommunalka"
names=read_html(url)%>%
html_nodes(xpath='//*[@class="mw-category-group"]') %>%
html_text(trim = TRUE) %>%
strsplit(split = "\n") %>%
unlist()
names=read_html(url
)
View(names)
## For the devel version:
require(devtools)
install_github("tidymodels/rsample")
library(rsample)
library(tidyverse)
library(gutenbergr)
titles <- c(
"The War of the Worlds",
"Pride and Prejudice"
)
books <- gutenberg_works(title %in% titles) %>%
gutenberg_download(meta_fields = "title") %>%
mutate(document = row_number())
library(tidytext)
tidy_books <- books %>%
unnest_tokens(word,text) %>%
group_by(word) %>%
filter(n()>10) %>%
ungroup()
books_split <- books %>%
select(document) %>%
initial_split()
View(books_split)
books_split[["id"]]
train_data <- training(books_split)
test_data <- testing(books_split)
View(train_data)
View(test_data)
source('~/datasciencecoursera/Capstone/Capstone train/ML_tidytext.R', echo=TRUE)
View(sparse_words)
source('~/datasciencecoursera/Capstone/Capstone train/ML_tidytext.R', echo=TRUE)
View(sparse_words)
sparse_words <- tidy_books %>%
count(document,word) %>%
inner_join(train_data) %>%
cast_sparse(document,word,n)
View(sparse_words)
?cast_sparse
dat <- data.frame(a = c("row1", "row1", "row2", "row2", "row2"),
b = c("col1", "col2", "col1", "col3", "col4"),
val = 1:5)
View(dat)
cast_sparse(dat, a, b)
cast_sparse(dat, a, b, val)
head(sparse_words)
rownames(sparse_words)
colnames(sparse_words)
word_rownames <- as.integer(rownames(sparse_words))
books_joined <- data_frame(document=word_rownames)
View(books_joined)
books_joined <- data_frame(document=word_rownames) %>%
left_join(books %>%
select(document,title))
View(books_joined)
install.packages("glmnet")
library(glmnet)
is_jane <- books_joined$title == "Pride and Prejudice"
model <- cv.glmnet(sparse_words, is_jane,
family = "binomial",
parallel = TRUE, keep = TRUE
)
View(model)
plot(model)
model$lambda
model$cvm
plot(model$glmnet.fit)
sample(1:6,1)
sample(1:6,1)
sample(1:6,1)
sample(1:6,1)
sample(1:6,1)
sample(1:6,1)
sample(1:6,1)
sample(1:6,1)
sample(1:6,1)
sample(1:6,1)
sample(1:6,1)
sample(1:6,1)
sample(1:6,1)
sample(1:6,1)
rep(1/6,6)
probability=rep(1\6,6)
probability=rep(1/6,6)
cum_probability=cumsum(probability)
sample(c("H","T"),1)
sample(c("H","T"),1)
sample(c("H","T"),1)
sample(c("H","T"),1)
sample(1:6,2)
sample(1:6,2)
sample(1:6,2)
sample(1:6,2)
sample(1:6,2)
sample(1:6,2)
sample(1:6,2)
dbinom(x=5,size = 10,prob = 0.5)
dbinom(x=5,size = 10,prob = 0.5)
dbinom(x=4:7,size = 10,prob = 0.5)
sum(dbinom(x=4:7,size = 10,prob = 0.5))
library(WDI)
install.packages('WDI')
library(WDI)
library(ggplot2)
library(googleVis)
library(plyr)
# World population total
population = WDI(indicator='SP.POP.TOTL', country="all",start=1960, end=2019)
# GDP in US $
gdp= WDI(indicator='NY.GDP.MKTP.CD', country="all",start=1960, end=2019)
# Life expectancy at birth (Years)
lifeExpectancy= WDI(indicator='SP.DYN.LE00.IN', country="all",start=1960, end=2019)
# GDP Per capita
income = WDI(indicator='NY.GDP.PCAP.PP.CD', country="all",start=1960, end=2019)
# Fertility rate (births per woman)
fertility = WDI(indicator='SP.DYN.TFRT.IN', country="all",start=1960, end=2019)
# Poverty head count
#poverty= WDI(indicator='SI.POV.2DAY', country="all",start=1960, end=2016)
poverty= WDI(indicator='SI.POV.NAHC', country="all",start=1960, end=2019)
View(fertility)
# Poverty head count
poverty= WDI(indicator='SI.POV.2DAY', country="all",start=1960, end=2016)
View(population)
names(population)[3]="Total population"
names(lifeExpectancy)[3]="Life Expectancy (Years)"
names(gdp)[3]="GDP (US$)"
names(income)[3]="GDP per capita income"
names(fertility)[3]="Fertility (Births per woman)"
#names(poverty)[3]="Poverty headcount ratio"
View(fertility)
j1 <- join(population, gdp)
j2 <- join(j1,lifeExpectancy)
j3 <- join(j2,income)
#j4 <- join(j3,poverty)
wbData <- join(j3,fertility)
View(wbData)
#This returns  list of 2 matrixes
wdi_data =WDI_data
# The 1st matrix is the list is the set of all World Bank Indicators
indicators=wdi_data[[1]]
# The 2nd  matrix gives the set of countries and regions
countries=wdi_data[[2]]
df = as.data.frame(countries)
aa <- df$region != "Aggregates"
# Remove the aggregates
countries_df <- df[aa,]
# Subset from the development data only those corresponding to the countries
bb = subset(wbData, country %in% countries_df$country)
cc = join(bb,countries_df)
dd = complete.cases(cc)
developmentDF = cc[dd,]
View(bb)
View(indicators)
indicators.head()
head(indicators)
WDI_data
head(WDI_data)
gg<- gvisMotionChart(cc,
idvar = "country",
timevar = "year",
xvar = "GDP (US$)",
yvar = "Life Expectancy (Years)",
sizevar ="Total population",
colorvar = "region")
plot(gg)
cat(gg$html$chart, file="WorldBank_chart1.html")
names(population)[3]="Total population"
names(gdp)[3]="GDP US($)"
names(elecAccess)[3]="Access to Electricity (% popn)"
# World population
population = WDI(indicator='SP.POP.TOTL', country="all",start=1960, end=2019)
# GDP in US $
gdp= WDI(indicator='NY.GDP.MKTP.CD', country="all",start=1960, end=2019)
# Access to electricity (% population)
elecAccess= WDI(indicator='EG.ELC.ACCS.ZS', country="all",start=1960, end=2019)
# Electric power consumption Kwh per capita
elecConsumption= WDI(indicator='EG.USE.ELEC.KH.PC', country="all",start=1960, end=2019)
#CO2 emissions
co2Emissions= WDI(indicator='EN.ATM.CO2E.KT', country="all",start=1960, end=2019)
# Access to sanitation (% population)
#sanitationAccess= WDI(indicator='SH.STA.ACSN', country="all",start=1960, end=2019)
sanitationAccess= WDI(indicator='SH.STA.BASS.ZS', country="all",start=1960, end=2019)
gg<- gvisMotionChart(cc,
idvar = "country",
timevar = "year",
xvar = "GDP (US$)",
yvar = "Life Expectancy (Years)",
sizevar ="Total population",
colorvar = "region")
plot(gg)
cat(gg$html$chart, file="WorldBank_chart1.html")
gg<- gvisMotionChart(cc,
idvar = "country",
timevar = "year",
xvar = "GDP (US$)",
yvar = "Life Expectancy (Years)",
sizevar ="Total population",
colorvar = "region")
plot(gg)
cat(gg$html$chart, file="WorldBank_chart1.html")
names(population)[3]="Total population"
names(gdp)[3]="GDP US($)"
names(elecAccess)[3]="Access to Electricity (% popn)"
names(elecConsumption)[3]="Electric power consumption (KWH per capita)"
names(co2Emissions)[3]="CO2 emisions"
names(sanitationAccess)[3]="Access to sanitation(% popn)"
j1 <- join(population, gdp)
j2 <- join(j1,elecAccess)
j3 <- join(j2,elecConsumption)
j4 <- join(j3,co2Emissions)
wbData1 <- join(j4,sanitationAccess)
#This returns  list of 2 matrixes
wdi_data =WDI_data
# The 1st matrix is the list is the set of all World Bank Indicators
indicators=wdi_data[[1]]
# The 2nd  matrix gives the set of countries and regions
countries=wdi_data[[2]]
df = as.data.frame(countries)
aa <- df$region != "Aggregates"
# Remove the aggregates
countries_df <- df[aa,]
# Subset from the development data only those corresponding to the countries
ee = subset(wbData1, country %in% countries_df$country)
ff = join(ee,countries_df)
gg1<- gvisMotionChart(ff,
idvar = "country",
timevar = "year",
xvar = "GDP US($)",
yvar = "Access to Electricity (% popn)",
sizevar ="Total population",
colorvar = "region")
plot(gg1)
cat(gg1$html$chart, file="WorldBank_chart2.html")
print(getwd())
getwd()
setwd("~/Python_Scripts/Ukr_budget/Shiny_app/population")
shiny::runApp()
df
#url<-"https://population.un.org/wpp/DVD/Files/1_Indicators%20(Standard)/CSV_FILES/WPP2017_TotalPopulationBySex.csv"
#download.file(url,dest='data/WPP2017_TotalPopulationBySex.csv')
df<- read.csv('data/WPP2017_TotalPopulationBySex.csv')
View(df)
runApp()
runApp()
runApp()
?scales
runApp()
runApp()
?option
options(digits = 15)
pi
options(scipen = 3); print(1e5)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
ggplot(data=df,
aes(x=Time,y=PopTotal,
col=Variant))+
geom_line()+
labs(y='Population,mln',x='Years')+
theme(axis.text.x = element_text(angle = 90, vjust=0.5))+
scale_x_continuous(breaks = seq(1950, 2100, by = 10))+
facet_grid(paste(Location, "~ ."))
ggplot(data=df,
aes(x=Time,y=PopTotal,
col=Variant))+
geom_line()+
labs(y='Population,mln',x='Years')+
theme(axis.text.x = element_text(angle = 90, vjust=0.5))+
scale_x_continuous(breaks = seq(1950, 2100, by = 10))+
facet_grid(paste(df$Location, "~ ."))
ggplot(data=df,
aes(x=Time,y=PopTotal,
col=Variant))+
geom_line()+
labs(y='Population,mln',x='Years')+
theme(axis.text.x = element_text(angle = 90, vjust=0.5))+
scale_x_continuous(breaks = seq(1950, 2100, by = 10))+
facet_grid(paste(unique(df$Location), "~ ."))
ggplot(data=df,
aes(x=Time,y=PopTotal,
col=Variant))+
geom_line()+
labs(y='Population,mln',x='Years')+
theme(axis.text.x = element_text(angle = 90, vjust=0.5))+
scale_x_continuous(breaks = seq(1950, 2100, by = 10))+
facet_grid(Location~.)
runApp()
runApp()
runApp()
